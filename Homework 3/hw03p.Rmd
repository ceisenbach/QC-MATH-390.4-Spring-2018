---
title: "HW03p"
author: "Chaim Eisenbach"
date: "April 13, 2018"
output: pdf_document
---

```{r setup, cache = F}
knitr::opts_chunk$set(error = TRUE) #this allows errors to be printed into the PDF
rm(list = ls())
```

1. Load pacakge `ggplot2` below using `pacman`.

```{r}
#TO-DO
pacman::p_load(ggplot2, quantreg)
```

The dataset `diamonds` is in the namespace now as it was loaded with the `ggplot2` package. Run the following code and write about the dataset below.

```{r}
data(diamonds)
?diamonds
str(diamonds)
```

What is $n$, $p$, what do the features mean, what is the most likely response metric and why?
n is 53940 and p is 10
Response metric would be price


Regardless of what you wrote above, the variable `price` will be the response variable going forward. 

Use `ggplot` to look at the univariate distributions of *all* predictors. Make sure you handle categorical predictors differently from continuous predictors.

```{r}
#TO-DO
ggplot(diamonds, aes(carat)) + geom_freqpoly()
ggplot(diamonds, aes(depth)) + geom_freqpoly()
ggplot(diamonds, aes(table)) + geom_freqpoly()
ggplot(diamonds, aes(price)) + geom_freqpoly()
ggplot(diamonds, aes(x)) + geom_freqpoly()
ggplot(diamonds, aes(y)) + geom_freqpoly()
ggplot(diamonds, aes(z)) + geom_freqpoly()
ggplot(diamonds, aes(color)) + geom_bar()
ggplot(diamonds, aes(cut)) + geom_bar()
ggplot(diamonds, aes(clarity)) + geom_bar()
```

Use `ggplot` to look at the bivariate distributions of the response versus *all* predictors. Make sure you handle categorical predictors differently from continuous predictors. This time employ a for loop when an logic that handles the predictor type.

```{r}
#TO-DO
for (i in 1:10){
  j = as.vector(as.matrix(diamonds[,i]))
  
  if(typeof(j) == "character"){
   print(ggplot(diamonds,aes(x = j, y = price)) +  xlab(colnames(diamonds[,i]))+ geom_bar(stat = "identity") )
  } else { print(ggplot(diamonds,aes(x = j, y = price)) + xlab(colnames(diamonds[,i]))+geom_point(shape = 11) + geom_smooth()) 
  }

}

```

Does depth appear to be mostly independent of price?
Yes
**TO-DO

Look at depth vs price by predictors cut (using faceting) and color (via different colors).

```{r}
#TO-DO

graph1 = ggplot(diamonds, aes(x = z, y = price)) +
  ggtitle("Price by Depth, Cut & Color", subtitle = "in the diamonds dataset") +
  ylab("Price (in $'s)") +
  xlab("depth") +
   xlim(0, 10) +
   
  geom_point(aes(col = color)) +scale_color_brewer(type = "div")+
  facet_grid(. ~ cut)
  
graph1
```


Does diamond color appear to be independent of diamond depth?
Yes


Does diamond cut appear to be independent of diamond depth?
Not necessarily


Do these plots allow you to assess well if diamond cut is independent of diamond price? Yes / no
No


We never discussed in class bivariate plotting if both variables were categorical. Use the geometry "jitter" to visualize color vs clarity. visualize price using different colors. Use a small sized dot.

```{r}
#TO-DO
graph2 = ggplot(diamonds, aes(x = clarity, y = color)) +
  ggtitle("Color by Clarity", subtitle = "in the diamonds dataset") +
  ylab("color") +
  xlab("clarity") +
  geom_point(aes(col = price)) +
  
  geom_jitter(size = .5)
graph2  
```

Does diamond clarity appear to be mostly independent of diamond color?
yes


2. Use `lm` to run a least squares linear regression using depth to explain price. 

```{r}

mod = lm(price ~  depth, diamonds)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 


```{r}

coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
sd(diamonds$price)
```

Are these metrics expected given the appropriate or relevant visualization(s) above?
Yes, we saw price is probably independant of depth.
**TO-DO

Use `lm` to run a least squares linear regression using carat to explain price. 

```{r}
#TO-DO
mod2 = lm(price ~  carat, diamonds)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 

```{r}
#TO-DO
coef(mod2)
summary(mod2)$r.squared
summary(mod2)$sigma
sd(diamonds$price)
```

Are these metrics expected given the appropriate or relevant visualization(s) above?
Yes
**TO-DO

3. Use `lm` to run a least squares anova model using color to explain price. 

```{r}
#TO-DO
diamonds$color = factor(as.character(diamonds$color))
anova_mod1 = lm(price ~ color, diamonds)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 

```{r}
#TO-DO
coef(anova_mod1)
summary(anova_mod1)$r.squared
summary(anova_mod1)$sigma
sd(diamonds$price)
```

Are these metrics expected given the appropriate or relevant visualization(s) above?
Essentially
**TO-DO

Our model only included one feature - why are there more than two estimates in $b$?

We used a categorical predictor

**TO-DO

Verify that the least squares linear model fit gives the sample averages of each price given color combination. Make sure to factor in the intercept here.

```{r}
#TO-DO
b = 3169.954
abs(b - mean(diamonds$price[diamonds$color == "D"]) )  
abs(b - mean(diamonds$price[diamonds$color == "E"]))
abs(b -mean(diamonds$price[diamonds$color == "F"]))
abs(b -mean(diamonds$price[diamonds$color == "G"]))
abs(b -mean(diamonds$price[diamonds$color == "H"]))
abs(b -mean(diamonds$price[diamonds$color == "I"]))
abs(b -mean(diamonds$price[diamonds$color == "J"]))
```

Fit a new model without the intercept and verify the sample averages of each colors' prices *directly* from the entries of vector $b$.

```{r}
#TO-DO
anova_mod2 = lm(price ~ 0 + color, diamonds)
coef(anova_mod2)
mean(diamonds$price[diamonds$color == "D"])
mean(diamonds$price[diamonds$color == "E"])
mean(diamonds$price[diamonds$color == "F"])
mean(diamonds$price[diamonds$color == "G"])
mean(diamonds$price[diamonds$color == "H"])
mean(diamonds$price[diamonds$color == "I"])
mean(diamonds$price[diamonds$color == "J"])
```

What would extrapolation look like in this model? We never covered this in class explicitly.
You can't really extrapolate from a categorical predictor
**TO-DO

4. Use `lm` to run a least squares linear regression using all available features to explain diamond price. 

```{r}
#TO-DO
diamonds$cut = factor(as.character(diamonds$cut))
diamonds$clarity = factor(as.character(diamonds$clarity))
mod3 = lm(price ~ ., diamonds)
```

What is $b$, $R^2$ and the RMSE? Also - provide an approximate 95% interval for predictions using the empirical rule. 

```{r}
#TO-DO
coef(mod3)
summary(mod3)$r.squared
summary(mod3)$sigma
```

Interpret all entries in the vector $b$.
The intercept is color D, cut Fair, clarity I1. Each slope for each variable is the increase or decrease in price from that variable.
**TO-DO

Are these metrics expected given the appropriate or relevant visualization(s) above? Can you tell from the visualizations?
It's difficult to tell from the visualizations, except for carat
**TO-DO

Comment on why $R^2$ is high. Think theoretically about diamonds and what you know about them.
Knowing all the important factors that go into the price of a diamond, as opposed to knowing just one factor, enables us to more accurately assess the price.
**TO-DO

Do you think you overfit? Comment on why or why not but do not do any numerical testing or coding.

Not particularly, there is a large enough sample size that this model could probably fit any diamond we'd want to predict the price of.

**TO-DO

Create a visualization that shows the "original residuals" (i.e. the prices minus the average price) and the model residuals.

```{r}
#TO-DO
ggplot(data.frame(null_residuals = diamonds$price - mean(diamonds$price), residuals = resid(mod3))) + 
  stat_density(aes(x = residuals), fill = "darkgreen", alpha = 0.3) + 
  stat_density(aes(x = null_residuals, fill = "red", alpha = 0.3)) 
```


5. Reference your visualizations above. Does price vs. carat appear linear?

Yes-ish

** TO-DO

Upgrade your model in #4 to use one polynomial term for carat.

```{r}
#TO-DO
mod4 = lm(price ~ cut+color+clarity+x+y+z+depth+table +poly(carat, 2, raw = TRUE), diamonds)
```

What is $b$, $R^2$ and the RMSE? 

```{r}
#TO-DO
coef(mod4)
summary(mod4)$r.squared
summary(mod4)$sigma
```

Interpret each element in $b$ just like previously. You can copy most of the text from the previous question but be careful. There is one tricky thing to explain.
The intercept is color D, cut Fair, clarity I1. Each slope for each variable is the increase or decrease in price from that variable. carat^2 is negative because when we extrapolate a polynomial it will eventually go down.
**TO-DO

Is this an improvement over the model in #4? Yes/no and why.
Yes the rmse is lower and r-squared is higher
**TO-DO

Define a function $g$ that makes predictions given a vector of the same features in $\mathbb{D}$.

```{r}
#TO-DO
g = function(vec){
 y_hat =  predict(mod3, vec)
 y_hat
}
#n = 53000
#test_indices = sample(1 : n, 1)
#X_test = (diamonds[test_indices, ])
#X_test$price = NULL
#g(X_test)
```

6. Use `lm` to run a least squares linear regression using a polynomial of color of degree 2 to explain price.  

```{r}
degree_2_poly_mod1 = lm(price ~ poly(color, 2, raw = TRUE), diamonds)

```

Why did this throw an error?
Color is categorical
**TO-DO

7. Redo the model fit in #4 without using `lm` but using the matrix algebra we learned about in class. This is hard and requires many lines, but it's all in the notes.

```{r}
#Xmm = model.matrix(price ~ ., diamonds)
#head(Xmm, 20)

#TO-DO
data("diamonds")
diamonds$cutFair = ifelse(diamonds$cut =="Fair", 1,0)
diamonds$cutGood = ifelse(diamonds$cut =="Good", 1,0)
diamonds$cutVery_good = ifelse(diamonds$cut =="Very Good", 1,0)
diamonds$cutPremium = ifelse(diamonds$cut =="Premium", 1,0)
diamonds$cutIdeal = ifelse(diamonds$cut =="Ideal", 1,0)
diamonds$colorD = ifelse(diamonds$color =="D", 1,0)
diamonds$colorE = ifelse(diamonds$color =="E", 1,0)
diamonds$colorE = ifelse(diamonds$color =="F", 1,0)
diamonds$colorH = ifelse(diamonds$color =="H", 1,0)
diamonds$colorI = ifelse(diamonds$color =="I", 1,0)
diamonds$colorJ = ifelse(diamonds$color =="J", 1,0)
diamonds$clarityIF = ifelse(diamonds$clarity =="IF",1,0)
diamonds$clarityVVS1 = ifelse(diamonds$clarity =="VVS1", 1,0)
diamonds$clarityVVS2 = ifelse(diamonds$clarity =="VVS2", 1,0)
diamonds$clarityVS1 = ifelse(diamonds$clarity =="VS1", 1,0)
diamonds$clarityVS2 = ifelse(diamonds$clarity =="VS2", 1,0)
diamonds$claritySI1 = ifelse(diamonds$clarity =="SI1", 1,0)
diamonds$claritySI2 = ifelse(diamonds$clarity =="SI2", 1,0)
diamonds$cut = NULL
diamonds$color = NULL
diamonds$clarity = NULL

diamonds1 = diamonds[sample(nrow(diamonds), 0.08*nrow(diamonds)), ]
y = diamonds1$price
diamonds1$price = NULL
diamonds1$cutGood = NULL
diamonds$colorD = NULL
diamonds$clarityIF =NULL
X = as.matrix(cbind(1,diamonds1))
XtX = t(X) %*% X
XtXinv = solve(XtX, tol =  1.73e-22)
H = X %*% XtXinv %*% t(X)
yhat = H %*% y

head(yhat)
#diamonds2 = diamonds
#diamonds$cut = factor(as.numeric(diamonds$cut))
#diamonds$clarity = factor(as.numeric(diamonds$clarity))
#diamonds$color = factor(as.numeric(diamonds$color))
#mod7 = lm(price ~ ., diamonds)
#coef(mod7)
#summary(mod7)$r.squared
#summary(mod7)$sigma
#diamonds = diamonds[sample(nrow(diamonds), 0.08*nrow(diamonds)), ]
#diamonds$price = NULL

#X = as.matrix(cbind(1, diamonds))
#XtX = t(X) %*% X
#XtXinv = solve(XtX, tol =  1.73e-40)
#H = X %*% XtXinv %*% t(X)

#yhat = H %*% y

#head(yhat)

#diamonds1 = diamonds
#diamonds1 =  diamonds1[sample(nrow(diamonds1), 1000), ]
#X1 = as.matrix(cbind(1, diamonds1))

#XtX = t(X1) %*% X1
#XtXinv = solve(XtX, tol = 1e-26)
#H = X1 %*% XtXinv %*% t(X1)
#warnings()
#yhat = H %*% y
#head(yhat)
```

What is $b$, $R^2$ and the RMSE? 

```{r}
#TO-DO
e = y - yhat
#head(e)
I = diag(nrow(X))
e_with_H = (I - H) * y
#head(e_with_H)
ybar = mean(y)
SST = sum((y - ybar)^2)
SSR = sum((yhat - ybar)^2)
SSE = sum(e^2)
#SSR + SSE
#SST
b = XtXinv %*% t(X) %*% y
b #b
(SST -SSE)/SST #R^2
sqrt(mean(e^2)) #RMSE
```

Are they the same as in #4?
Very close, but we sampled so it's not the same
**TO-DO

Redo the model fit using matrix algebra by projecting onto an orthonormal basis for the predictor space $Q$ and the Gram-Schmidt "remainder" matrix $R$. Formulas are in the notes. Verify $b$ is the same.

```{r}
#TO-DO
qrX = qr(X)
Q = qr.Q(qrX)
R = qr.R(qrX)

sum(Q[, 1]^2) #normalized?
sum(Q[, 2]^2) #normalized?
Q[, 1] %*% Q[, 2] #orthogonal?
Q[, 2] %*% Q[, 3] #orthogonal?

yhat_via_Q = Q %*% t(Q) %*% y
head(yhat)
head(yhat_via_Q)
```

Generate the vectors $\hat{y}$, $e$ and the hat matrix $H$.

```{r}
#TO-DO
e = y - yhat
H = X %*% XtXinv %*% t(X)
yhat = H %*% y
```

In one line each, verify that 
(a) $\hat{y}$ and $e$ sum to the vector $y$ (the prices in the original dataframe), 
(b) $\hat{y}$ and $e$ are orthogonal 
(c) $e$ projected onto the column space of $X$ gets annhilated, 
(d) $\hat{y}$ projected onto the column space of $X$ is unaffected, 
(e) $\hat{y}$ projected onto the orthogonal complement of the column space of $X$ is annhilated
(f) the sum of squares residuals plus the sum of squares model equal the original (total) sum of squares

```{r}
#TO-DO

pacman::p_load(testthat)
expect_equal(as.vector(e+yhat), y) #(a)
expect_equal(as.vector(t(yhat) %*% e),0, tol = 1e-1) #(b)
expect_equal(sum(H %*% e),0 , tol = 1e-4) #(c)
expect_equal (H %*% yhat, yhat) #(d)
expect_equal(sum((I-H) %*% yhat), 0 , tol = 1e-4) #(e)
expect_equal(SSR + SSE,SST) # (f)

```

8. Fit a linear least squares model for price using all interactions and also 5-degree polynomials for all continuous predictors.

```{r}
#TO-DO
#cut+color+clarity
data("diamonds")
mod5 = lm(price ~.*.  +poly(carat, 5, raw = TRUE)+poly(x, 5, raw = TRUE)+poly(y, 5, raw = TRUE)+poly(z, 5, raw = TRUE)+poly(depth, 5, raw = TRUE)+poly(table, 5, raw = TRUE), diamonds)
```

Report $R^2$, RMSE, the standard error of the residuals ($s_e$) but you do not need to report $b$.

```{r}
#TO-DO
summary(mod5)$r.squared
summary(mod5)$sigma
sd(mod5$residuals)
```

Create an illustration of $y$ vs. $\hat{y}$.

```{r}
#TO-DO
y =(diamonds$price)
y_hat = as.vector(y - resid(mod5))
ggplot() +
  stat_density(aes(x = y), fill = "darkgreen", alpha = 0.3) + 
  stat_density(aes(x = y_hat, fill = "red", alpha = 0.3)) 

```

How many diamonds have predictions that are wrong by \$1,000 or more ?

```{r}
#TO-DO
sum(abs(summary(mod5$residuals))>=1000)
```

$R^2$ now is very high and very impressive. But is RMSE impressive? Think like someone who is actually using this model to e.g. purchase diamonds.
Not terrible but it's still + or - 1200$
**TO-DO

What is the degrees of freedom in this model?

```{r}
#TO-DO
b = coef(mod5)
b
```

Do you think $g$ is close to $h^*$ in this model? Yes / no and why?

**TO-DO

Do you think $g$ is close to $f$ in this model? Yes / no and why?

**TO-DO

What more degrees of freedom can you add to this model to make $g$ closer to $f$?

** TO-DO

Even if you allowed for so much expressivity in $\mathcal{H}$ that $f$ was an element in it, there would still be error due to ignorance of relevant information that you haven't measured. What information do you think can help? This is not a data science question - you have to think like someone who sells diamonds.

** TO-DO

9. Validate the model in #8 by reserving 10% of $\mathbb{D}$ as test data. Report oos standard error of the residuals

```{r}
#TO-DO
#set.seed(1000)
n = 53940
K = 10 #i.e. the test set is 1/10th of the entire historical dataset
y = diamonds$price
#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = diamonds[train_indices, ]
y_train = y[train_indices]
X_test = diamonds[test_indices, ]
y_test = y[test_indices]
mod6 = lm(y_train ~ cut+color+clarity+poly(carat, 5, raw = TRUE)+poly(x, 5, raw = TRUE)+poly(y, 5, raw = TRUE)+poly(z, 5, raw = TRUE)+poly(depth, 5, raw = TRUE)+poly(table, 5, raw = TRUE), data.frame(X_train))
summary(mod6)$r.squared
sd(mod6$residuals)
y_hat_oos = predict(mod6, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```

Compare the oos standard error of the residuals to the standard error of the residuals you got in #8 (i.e. the in-sample estimate). Do you think there's overfitting?
no
** TO-DO

Extra-credit: validate the model via cross validation.

```{r}
#TO-DO if you want extra credit
```

Is this result much different than the single validation? And, again, is there overfitting in this model?

** TO-DO

10. The following code (from plec 14) produces a response that is the result of a linear model of one predictor and random $\epsilon$.

```{r}
rm(list = ls())
set.seed(1004)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
x = runif(n, xmin, xmax)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

We then add fake predictors. For instance, here is the model with the addition of 2 fake predictors:

```{r}
p_fake = 100
X = matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake)
mod = lm(y ~ X)
summary(mod)$r.squared
sd(mod$residuals)
```

Using a test set hold out, find the number of fake predictors where you can reliably say "I overfit". Some example code is below that you may want to use:

```{r}


set.seed(1003)
#TO-DO
p_fake = 50
X = matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake)
mod = lm(y ~ X)
summary(mod)$r.squared
sd(mod$residuals)


K = 10

test_indices = sample(1 : n, 1 / 10 * n)
train_indices = setdiff(1 : n, test_indices)
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]
mod = lm(y_train ~.,data.frame(X_train))
summary(mod)$r.squared
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```
